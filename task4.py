# -*- coding: utf-8 -*-
"""Copie de Copie de Seance 3 - TP DATA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K9iZ5_xE4PrlYRxLxyLtCgtfplu-wI8M

# Third session - To start solving the fourth tasks of the proposed project!

The last task, the one that we will devote the most of our time, is to use the dataset to perform an activtity recognition task. That means that, given a sample of the dataset your code must be able to recognize from which of the 10 types of activity it came from, and the accuracy should be measurable beyond simple showing the confusion matrix.

Following the results from the previous session, we should have a complete segmented dataset. The next step is to group all activities in a single dataset and label them the name of the activities in the activites file. This could be done by sligthly altering the code used to segment the data adding the correponding number of the activity to the last column. The order of the activities here.

As a suggesstion, if you used the code from the previous notebooks as a start to your segmentation, the modification to the code can be as simple as the piece of code below:
"""

from google.colab import drive
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

drive.mount('/content/drive')

# Path to the Excel file that contains the activity calendar
activity_file_path = '/content/drive/My Drive/Projet Data Science/TP Data 2024 - FISA/activites.xlsx'

# Load the specific sheet which contains the activity details
calendar = pd.read_excel(activity_file_path, sheet_name='Done so far', usecols=['activity', 'Started', 'Ended'])

# Ensure the 'Started' and 'Ended' times are localized to the correct timezone
calendar['Started'] = pd.to_datetime(calendar['Started']).dt.tz_localize('UTC').dt.tz_convert('Europe/Paris')
calendar['Ended'] = pd.to_datetime(calendar['Ended']).dt.tz_localize('UTC').dt.tz_convert('Europe/Paris')
# Chemin vers le fichier de la base de données
file_path = "/content/drive/My Drive/Projet Data Science/TP Data 2024 - FISA/Piano/IMT_PICO.csv"

# Charger les données avec low_memory=False pour éviter l'avertissement de Dtype
base = pd.read_csv(file_path, sep=';', low_memory=False)

# Vérifier si la colonne 'Time' est déjà au bon fuseau horaire
if base['Time'].dtype == 'object':
    base['Time'] = pd.to_datetime(base['Time'], errors='coerce')
if base['Time'].dt.tz is None:
    base['Time'] = base['Time'].dt.tz_localize('UTC')
base['Time'] = base['Time'].dt.tz_convert('Europe/Paris')

# Prepare a DataFrame to hold the labeled dataset
labelled_dataset = pd.DataFrame()

# Define a dictionary to map activities to labels
activity_labels = {
    'AS1': 1,
    'Oeuf': 2,
    'SdB': 3,
    'Nett': 4,
    'Aera': 5,
    'Saber': 6,
    'Bougie': 7,
    'BricoP': 8,
    'BricoC': 9,
    'AS2': 10
}

# Handle activities not found in the dictionary
default_label = 0

# Iterate over the activity calendar to segment and label the dataset
for idact, act in enumerate(calendar['activity']):
    if act in activity_labels:
        start = calendar['Started'][idact]
        end = calendar['Ended'][idact]
        activity_data = base[(base['Time'] >= start) & (base['Time'] <= end)]
        activity_data = activity_data.reset_index(drop=True).sort_values(by='Time').drop(columns='Time')
        activity_data['label'] = activity_labels.get(act, default_label) * np.ones(len(activity_data))  # Set label
        labelled_dataset = pd.concat([labelled_dataset, activity_data], ignore_index=True)

# Save the fully labelled dataset to a CSV file
labelled_dataset.to_csv('/content/drive/My Drive/Projet Data Science/TP Data 2024 - FISA/labelled_dataset.csv', index=False, sep=';')

# Load the data
data = pd.read_csv('/content/drive/My Drive/Projet Data Science/TP Data 2024 - FISA/labelled_dataset.csv', sep=';')

# Separate features and target label
X = data.drop('label', axis=1)
y = data['label']

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""Them you concatenate all activities and save it to a .csv file that is going to be loaded into the pytorch environment in which the machine learning is going to take place.

To load the custom dataset into the pytorch environment the piece of code below can be used:
"""

from sklearn.preprocessing import RobustScaler
from torch.utils.data import Dataset, DataLoader
import torch
import torch.optim as optim

def robust_transform(features):
    scaler = RobustScaler()
    features = scaler.fit_transform(features)
    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)
    return features

class CustomDataset(Dataset):
    def __init__(self, csv_file, transform=None):
        self.data_frame = pd.read_csv(csv_file, sep=';')
        self.data_frame.drop(columns=['Unnamed: 15'], inplace=True)
        self.features = self.data_frame.drop('label', axis=1).values
        self.labels = self.data_frame['label'].values

        if transform:
            self.features = transform(self.features)

    def __len__(self):
        return len(self.data_frame)

    def __getitem__(self, idx):
        features = torch.tensor(self.features[idx], dtype=torch.float32)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return features, label

# Normalization of features
scaler = StandardScaler()

csv_file = "/content/drive/My Drive/Projet Data Science/TP Data 2024 - FISA/labelled_dataset.csv"

# Create an instance of your custom dataset with scaling transformation
custom_dataset = CustomDataset(csv_file, transform=robust_transform)

if np.any(np.isnan(custom_dataset.features)) or np.any(np.isinf(custom_dataset.features)):
    print("NaN or Inf found in features after transformation")

batch_size = 64

# Split the dataset into train and test sets
train_size = int(0.8 * len(custom_dataset))
test_size = len(custom_dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, test_size])

# Create DataLoaders for training and testing
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

print("Data loaders created and ready for training and testing!")

"""Then, you can the codes provided from the previous TPs to create your machine learning layers.

The suggestions in the presentation were: K-NN, LSTM, and CNNs, but you can use whatever neural network architecture (even a simple feedforward neural net) you want, as long as the classification result is measurable and the process well explained.
"""

import torch
import torch.nn as nn
import torch.optim as optim

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR

class EnhancedNN(nn.Module):
    def __init__(self):
        super(EnhancedNN, self).__init__()
        self.layer1 = nn.Linear(10, 64)
        self.relu = nn.ReLU()
        self.dropout1 = nn.Dropout(0.5)
        self.layer2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.3)
        self.layer3 = nn.Linear(32, 16)
        self.output_layer = nn.Linear(16, 10)

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.dropout1(x)
        x = self.relu(self.layer2(x))
        x = self.dropout2(x)
        x = self.relu(self.layer3(x))
        x = self.output_layer(x)
        return x

model = EnhancedNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0015)
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

def train_model(model, criterion, optimizer, train_loader, test_loader, epochs=75):
    best_accuracy = 0
    patience = 10
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        scheduler.step()
        val_accuracy = validate_model(model, test_loader)
        print(f'Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}, Validation Accuracy: {val_accuracy} %')

        # Early stopping
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print("Stopping early due to lack of improvement")
            break

def validate_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in test_loader:
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    accuracy = 100 * correct / total
    return accuracy

train_model(model, criterion, optimizer, train_loader, test_loader)
accuracy = validate_model(model, test_loader)

print(f'ACCURACY : {accuracy}')